{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyspark in c:\\python312\\lib\\site-packages (3.5.3)\n",
      "Requirement already satisfied: torch in c:\\python312\\lib\\site-packages (2.4.1)\n",
      "Requirement already satisfied: tensorflow in c:\\python312\\lib\\site-packages (2.18.0)\n",
      "Requirement already satisfied: py4j==0.10.9.7 in c:\\python312\\lib\\site-packages (from pyspark) (0.10.9.7)\n",
      "Requirement already satisfied: filelock in c:\\python312\\lib\\site-packages (from torch) (3.16.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\python312\\lib\\site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: sympy in c:\\python312\\lib\\site-packages (from torch) (1.13.2)\n",
      "Requirement already satisfied: networkx in c:\\python312\\lib\\site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\python312\\lib\\site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\python312\\lib\\site-packages (from torch) (2024.9.0)\n",
      "Requirement already satisfied: setuptools in c:\\python312\\lib\\site-packages (from torch) (74.1.2)\n",
      "Requirement already satisfied: tensorflow-intel==2.18.0 in c:\\python312\\lib\\site-packages (from tensorflow) (2.18.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\python312\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (2.1.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\python312\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in c:\\python312\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (24.3.25)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in c:\\python312\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (0.6.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\python312\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (0.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\python312\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (18.1.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\python312\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (3.4.0)\n",
      "Requirement already satisfied: packaging in c:\\python312\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (24.1)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in c:\\python312\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (5.28.3)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\python312\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (2.32.3)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\python312\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\python312\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (2.5.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\python312\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (1.16.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\python312\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (1.68.0)\n",
      "Requirement already satisfied: tensorboard<2.19,>=2.18 in c:\\python312\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (2.18.0)\n",
      "Requirement already satisfied: keras>=3.5.0 in c:\\python312\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (3.6.0)\n",
      "Requirement already satisfied: numpy<2.1.0,>=1.26.0 in c:\\python312\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (2.0.0)\n",
      "Requirement already satisfied: h5py>=3.11.0 in c:\\python312\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (3.12.1)\n",
      "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in c:\\python312\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (0.4.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\python312\\lib\\site-packages (from jinja2->torch) (2.1.5)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\python312\\lib\\site-packages (from sympy->torch) (1.3.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\python312\\lib\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.18.0->tensorflow) (0.45.0)\n",
      "Requirement already satisfied: rich in c:\\python312\\lib\\site-packages (from keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow) (13.9.4)\n",
      "Requirement already satisfied: namex in c:\\python312\\lib\\site-packages (from keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow) (0.0.8)\n",
      "Requirement already satisfied: optree in c:\\python312\\lib\\site-packages (from keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow) (0.13.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\python312\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.18.0->tensorflow) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\python312\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.18.0->tensorflow) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\python312\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.18.0->tensorflow) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\python312\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.18.0->tensorflow) (2024.8.30)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\python312\\lib\\site-packages (from tensorboard<2.19,>=2.18->tensorflow-intel==2.18.0->tensorflow) (3.7)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\python312\\lib\\site-packages (from tensorboard<2.19,>=2.18->tensorflow-intel==2.18.0->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\python312\\lib\\site-packages (from tensorboard<2.19,>=2.18->tensorflow-intel==2.18.0->tensorflow) (3.1.3)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\python312\\lib\\site-packages (from rich->keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\tosha\\appdata\\roaming\\python\\python312\\site-packages (from rich->keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow) (2.18.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\python312\\lib\\site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow) (0.1.2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "# Install PySpark and related dependencies\n",
    "!pip install pyspark torch tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, udf, to_date, unix_timestamp\n",
    "from pyspark.ml.feature import StringIndexer, MinMaxScaler, VectorAssembler\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"CreditCardFraudDetection\") \\\n",
    "    .config(\"spark.executor.memory\", \"4g\") \\\n",
    "    .config(\"spark.driver.memory\", \"4g\") \\\n",
    "    .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _c0: integer (nullable = true)\n",
      " |-- trans_date_trans_time: timestamp (nullable = true)\n",
      " |-- cc_num: long (nullable = true)\n",
      " |-- merchant: string (nullable = true)\n",
      " |-- category: string (nullable = true)\n",
      " |-- amt: double (nullable = true)\n",
      " |-- first: string (nullable = true)\n",
      " |-- last: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- street: string (nullable = true)\n",
      " |-- city: string (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      " |-- zip: integer (nullable = true)\n",
      " |-- lat: double (nullable = true)\n",
      " |-- long: double (nullable = true)\n",
      " |-- city_pop: integer (nullable = true)\n",
      " |-- job: string (nullable = true)\n",
      " |-- dob: date (nullable = true)\n",
      " |-- trans_num: string (nullable = true)\n",
      " |-- unix_time: integer (nullable = true)\n",
      " |-- merch_lat: double (nullable = true)\n",
      " |-- merch_long: double (nullable = true)\n",
      " |-- is_fraud: integer (nullable = true)\n",
      "\n",
      "root\n",
      " |-- _c0: integer (nullable = true)\n",
      " |-- trans_date_trans_time: timestamp (nullable = true)\n",
      " |-- cc_num: long (nullable = true)\n",
      " |-- merchant: string (nullable = true)\n",
      " |-- category: string (nullable = true)\n",
      " |-- amt: double (nullable = true)\n",
      " |-- first: string (nullable = true)\n",
      " |-- last: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- street: string (nullable = true)\n",
      " |-- city: string (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      " |-- zip: integer (nullable = true)\n",
      " |-- lat: double (nullable = true)\n",
      " |-- long: double (nullable = true)\n",
      " |-- city_pop: integer (nullable = true)\n",
      " |-- job: string (nullable = true)\n",
      " |-- dob: date (nullable = true)\n",
      " |-- trans_num: string (nullable = true)\n",
      " |-- unix_time: integer (nullable = true)\n",
      " |-- merch_lat: double (nullable = true)\n",
      " |-- merch_long: double (nullable = true)\n",
      " |-- is_fraud: integer (nullable = true)\n",
      "\n",
      "+-------+-----------------+--------------------+-------------------+-------------+-----------------+-------+-------+-------+--------------------+-------+-------+-----------------+-----------------+------------------+-----------------+------------------+--------------------+--------------------+------------------+------------------+--------------------+\n",
      "|summary|              _c0|              cc_num|           merchant|     category|              amt|  first|   last| gender|              street|   city|  state|              zip|              lat|              long|         city_pop|               job|           trans_num|           unix_time|         merch_lat|        merch_long|            is_fraud|\n",
      "+-------+-----------------+--------------------+-------------------+-------------+-----------------+-------+-------+-------+--------------------+-------+-------+-----------------+-----------------+------------------+-----------------+------------------+--------------------+--------------------+------------------+------------------+--------------------+\n",
      "|  count|          1296675|             1296675|            1296675|      1296675|          1296675|1296675|1296675|1296675|             1296675|1296675|1296675|          1296675|          1296675|           1296675|          1296675|           1296675|             1296675|             1296675|           1296675|           1296675|             1296675|\n",
      "|   mean|         648337.0|4.171920420797207...|               NULL|         NULL|70.35103545607046|   NULL|   NULL|   NULL|                NULL|   NULL|   NULL|48800.67109722945|38.53762161490217|-90.22633537864573|88824.44056297839|              NULL|            Infinity|1.3492436367261226E9|38.537338044699666|-90.22646479897256|0.005788651743883394|\n",
      "| stddev|374317.9744882685|1.308806447000243...|               NULL|         NULL|160.3160385715277|   NULL|   NULL|   NULL|                NULL|   NULL|   NULL|26893.22247648588|5.075808438803937|13.759076946486305|301956.3606887509|              NULL|                 NaN|1.2841278423361162E7|  5.10978836967917|13.771090564792418| 0.07586268973125161|\n",
      "|    min|                0|         60416207185|fraud_Abbott-Rogahn|entertainment|              1.0|  Aaron| Abbott|      F|  000 Jennifer Mills|Achille|     AK|             1257|          20.0271|         -165.6723|               23|Academic librarian|00000ecad06b03d3a...|          1325376018|         19.027785|       -166.671242|                   0|\n",
      "|    max|          1296674| 4992346398065154184|   fraud_Zulauf LLC|       travel|          28948.9|Zachary| Zuniga|      M|99736 Rose Shoals...|Zavalla|     WY|            99783|          66.6933|          -67.9503|          2906700|            Writer|ffffef9d89e7d02d8...|          1371816817|         67.510267|        -66.950902|                   1|\n",
      "+-------+-----------------+--------------------+-------------------+-------------+-----------------+-------+-------+-------+--------------------+-------+-------+-----------------+-----------------+------------------+-----------------+------------------+--------------------+--------------------+------------------+------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load datasets\n",
    "train_df = spark.read.csv(\"Train.csv\", header=True, inferSchema=True)\n",
    "test_df = spark.read.csv(\"Test.csv\", header=True, inferSchema=True)\n",
    "\n",
    "# Display schema\n",
    "train_df.printSchema()\n",
    "test_df.printSchema()\n",
    "\n",
    "# Basic statistics\n",
    "train_df.describe().show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------------------------------------+--------------------------------------------------------------------------------------------------------------------------+\n",
      "|features_num                                                   |scaled_features                                                                                                           |\n",
      "+---------------------------------------------------------------+--------------------------------------------------------------------------------------------------------------------------+\n",
      "|[4.97,36.0788,-81.1781,36.011293,-82.048315,3495.0]            |[1.3714293610244608E-4,0.34396843968439694,0.8646384642148135,0.35030195030031674,0.8486024716722786,0.001194491166373147]|\n",
      "|[107.23,48.8878,-118.2105,49.159046999999994,-118.186462,149.0]|[0.003669696247396184,0.6184497559261308,0.48568183213605953,0.6214876127835203,0.4862075279727285,4.33484697474126E-5]   |\n",
      "|[220.11,42.1808,-112.262,43.150704,-112.154481,4154.0]         |[0.0075691155489690095,0.4747268901260442,0.5465534884672848,0.4975594896317395,0.5466965014359156,0.0014212105438615986] |\n",
      "|[45.0,46.2306,-112.1138,47.034331,-112.561071,1939.0]          |[0.001519972087785297,0.5615091865204368,0.5480700354065615,0.577663206269019,0.5426191988515082,6.591719685400201E-4]    |\n",
      "|[41.96,38.4207,-79.4629,38.674999,-78.632459,99.0]             |[0.0014149558344474036,0.394152512953701,0.8821902949182374,0.40524356818200846,0.8828568274035167,2.6146696038121884E-5] |\n",
      "+---------------------------------------------------------------+--------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import VectorAssembler, MinMaxScaler\n",
    "\n",
    "# Assemble numerical columns into a single feature vector\n",
    "numerical_cols = ['amt', 'lat', 'long', 'merch_lat', 'merch_long', 'city_pop']\n",
    "assembler = VectorAssembler(inputCols=numerical_cols, outputCol=\"features_num\")\n",
    "\n",
    "# Transform the data using VectorAssembler\n",
    "train_df = assembler.transform(train_df)\n",
    "test_df = assembler.transform(test_df)\n",
    "\n",
    "# Scale the numerical features\n",
    "scaler = MinMaxScaler(inputCol=\"features_num\", outputCol=\"scaled_features\")\n",
    "scaler_model = scaler.fit(train_df)\n",
    "\n",
    "train_df = scaler_model.transform(train_df)\n",
    "test_df = scaler_model.transform(test_df)\n",
    "\n",
    "# Verify the resulting DataFrame\n",
    "train_df.select(\"features_num\", \"scaled_features\").show(5, truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split train data into train and validation\n",
    "train_data, val_data = train_df.randomSplit([0.8, 0.2], seed=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:307: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    }
   ],
   "source": [
    "class PyTorchTransformer(nn.Module):\n",
    "    def __init__(self, input_dim, num_heads, num_layers, num_classes):\n",
    "        super(PyTorchTransformer, self).__init__()\n",
    "\n",
    "        # Adjust input_dim to ensure divisibility\n",
    "        assert input_dim % num_heads == 0, \"input_dim must be divisible by num_heads\"\n",
    "\n",
    "        # Transformer Encoder Layer\n",
    "        self.encoder_layer = TransformerEncoderLayer(d_model=input_dim, nhead=num_heads)\n",
    "        self.transformer = TransformerEncoder(self.encoder_layer, num_layers=num_layers)\n",
    "\n",
    "        # Fully connected output layer\n",
    "        self.fc = nn.Linear(input_dim, num_classes)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.transformer(x)  # Pass through the transformer\n",
    "        x = x.mean(dim=1)        # Global average pooling\n",
    "        x = self.fc(x)           # Fully connected layer\n",
    "        return self.softmax(x)\n",
    "\n",
    "# Model hyperparameters\n",
    "input_dim = len(numerical_cols)  # Ensure input_dim is divisible by num_heads\n",
    "num_heads = 2                    # Number of attention heads (divisor of input_dim)\n",
    "num_layers = 2                   # Number of Transformer layers\n",
    "num_classes = 2                  # Fraud (1) or Not Fraud (0)\n",
    "\n",
    "# Instantiate the model\n",
    "model = PyTorchTransformer(input_dim=input_dim, num_heads=num_heads, num_layers=num_layers, num_classes=num_classes)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert Spark DataFrame to Pandas\n",
    "train_pandas = train_data.select(\"scaled_features\", \"is_fraud\").toPandas()\n",
    "val_pandas = val_data.select(\"scaled_features\", \"is_fraud\").toPandas()\n",
    "\n",
    "# Prepare PyTorch datasets\n",
    "def to_tensor(df):\n",
    "    X = np.array(df[\"scaled_features\"].tolist())\n",
    "    y = df[\"is_fraud\"].values\n",
    "    return torch.tensor(X, dtype=torch.float32), torch.tensor(y, dtype=torch.long)\n",
    "\n",
    "X_train, y_train = to_tensor(train_pandas)\n",
    "X_val, y_val = to_tensor(val_pandas)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "val_dataset = TensorDataset(X_val, y_val)\n",
    "\n",
    "# Create DataLoaders for batching\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_transformer(model, train_loader, val_loader, optimizer, criterion, epochs=10, device='cpu'):\n",
    "    model.to(device)\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        \n",
    "        # Training loop\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(X_batch)\n",
    "            loss = criterion(outputs, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "        \n",
    "        # Validation loop\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for X_batch, y_batch in val_loader:\n",
    "                X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "                val_outputs = model(X_batch)\n",
    "                val_loss += criterion(val_outputs, y_batch).item()\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}, Train Loss: {train_loss/len(train_loader):.4f}, Val Loss: {val_loss/len(val_loader):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For independent transactions (sequence_length = 1)\n",
    "X_train = X_train.view(-1, 1, input_dim)  # Shape: (num_samples, sequence_length, input_dim)\n",
    "X_val = X_val.view(-1, 1, input_dim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: torch.Size([1037731, 1, 6])\n",
      "X_val shape: torch.Size([258944, 1, 6])\n"
     ]
    }
   ],
   "source": [
    "print(\"X_train shape:\", X_train.shape)  # Should be (num_samples, sequence_length, input_dim)\n",
    "print(\"X_val shape:\", X_val.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Create a DataLoader for batch processing\n",
    "batch_size = 128  # Adjust batch size based on available memory\n",
    "train_dataset = TensorDataset(X_train)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Extract features in batches\n",
    "train_features = []\n",
    "model.eval()  # Set model to evaluation mode\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in train_loader:\n",
    "        batch_features = model(batch[0]).detach().numpy()\n",
    "        train_features.append(batch_features)\n",
    "\n",
    "# Concatenate all batch features\n",
    "train_features = np.concatenate(train_features, axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using n_components for PCA: 6\n",
      "Reduced X_train shape: (1037731, 6)\n",
      "Reduced X_val shape: (258944, 6)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Reshape the 3D tensor into 2D\n",
    "n_samples, sequence_length, input_dim = X_train.shape\n",
    "X_train_flattened = X_train.reshape(n_samples, -1)  # Flatten to [n_samples, sequence_length * input_dim]\n",
    "X_val_flattened = X_val.reshape(X_val.shape[0], -1)\n",
    "\n",
    "# Dynamically set n_components for PCA\n",
    "n_components = min(128, min(X_train_flattened.shape[0], X_train_flattened.shape[1]))\n",
    "print(f\"Using n_components for PCA: {n_components}\")\n",
    "\n",
    "# Apply PCA to reduce dimensions\n",
    "pca = PCA(n_components=n_components)\n",
    "X_train_reduced = pca.fit_transform(X_train_flattened)\n",
    "X_val_reduced = pca.transform(X_val_flattened)\n",
    "\n",
    "print(\"Reduced X_train shape:\", X_train_reduced.shape)\n",
    "print(\"Reduced X_val shape:\", X_val_reduced.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "X_train = X_train.to(device)\n",
    "X_val = X_val.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.half()\n",
    "X_val = X_val.half()\n",
    "model = model.half()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Move data and model to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "X_train = X_train.to(device)\n",
    "X_val = X_val.to(device)\n",
    "\n",
    "# Batch processing\n",
    "batch_size = 128\n",
    "train_dataset = TensorDataset(X_train)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "train_features = []\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in train_loader:\n",
    "        batch_features = model(batch[0]).cpu().detach().numpy()  # Move to CPU\n",
    "        train_features.append(batch_features)\n",
    "\n",
    "# Concatenate results\n",
    "train_features = np.concatenate(train_features, axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------------+-----------+---------+\n",
      "|merchant_index|category_index|state_index|job_index|\n",
      "+--------------+--------------+-----------+---------+\n",
      "|         571.0|          11.0|       12.0|    141.0|\n",
      "|          81.0|           1.0|       29.0|     61.0|\n",
      "|         319.0|           6.0|       45.0|    457.0|\n",
      "|          40.0|           0.0|       39.0|    232.0|\n",
      "|         519.0|          10.0|       14.0|    297.0|\n",
      "+--------------+--------------+-----------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "# List of categorical columns\n",
    "categorical_cols = ['merchant', 'category', 'state', 'job']\n",
    "\n",
    "# StringIndexer to encode categorical variables\n",
    "indexers = [StringIndexer(inputCol=col, outputCol=col+\"_index\") for col in categorical_cols]\n",
    "\n",
    "# Apply StringIndexer to data\n",
    "from pyspark.ml import Pipeline\n",
    "pipeline = Pipeline(stages=indexers)\n",
    "train_df = pipeline.fit(train_df).transform(train_df)\n",
    "test_df = pipeline.fit(test_df).transform(test_df)\n",
    "\n",
    "# Verify encoded columns\n",
    "train_df.select(*[col+\"_index\" for col in categorical_cols]).show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------------+\n",
      "|   amt|          amt_scaled|\n",
      "+------+--------------------+\n",
      "|  4.97|[1.37142936102446...|\n",
      "|107.23|[0.00366969624739...|\n",
      "|220.11|[0.00756911554896...|\n",
      "|  45.0|[0.00151997208778...|\n",
      "| 41.96|[0.00141495583444...|\n",
      "+------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import MinMaxScaler, VectorAssembler\n",
    "\n",
    "# Assemble features for scaling\n",
    "scaler_assembler = VectorAssembler(inputCols=[\"amt\"], outputCol=\"amt_vector\")\n",
    "train_df = scaler_assembler.transform(train_df)\n",
    "test_df = scaler_assembler.transform(test_df)\n",
    "\n",
    "# Apply MinMaxScaler\n",
    "scaler = MinMaxScaler(inputCol=\"amt_vector\", outputCol=\"amt_scaled\")\n",
    "scaler_model = scaler.fit(train_df)\n",
    "\n",
    "train_df = scaler_model.transform(train_df)\n",
    "test_df = scaler_model.transform(test_df)\n",
    "\n",
    "# Drop unnecessary columns\n",
    "train_df = train_df.drop(\"amt_vector\")\n",
    "test_df = test_df.drop(\"amt_vector\")\n",
    "\n",
    "# Verify scaled column\n",
    "train_df.select(\"amt\", \"amt_scaled\").show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------------------+\n",
      "|features                                     |\n",
      "+---------------------------------------------+\n",
      "|[1.3714293610244608E-4,571.0,11.0,12.0,141.0]|\n",
      "|[0.003669696247396184,81.0,1.0,29.0,61.0]    |\n",
      "|[0.0075691155489690095,319.0,6.0,45.0,457.0] |\n",
      "|[0.001519972087785297,40.0,0.0,39.0,232.0]   |\n",
      "|[0.0014149558344474036,519.0,10.0,14.0,297.0]|\n",
      "+---------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# List of all feature columns (include scaled amt and encoded categorical features)\n",
    "feature_cols = [\"amt_scaled\"] + [col + \"_index\" for col in categorical_cols]\n",
    "\n",
    "# Assemble all features into a single vector\n",
    "assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features\")\n",
    "train_df = assembler.transform(train_df)\n",
    "test_df = assembler.transform(test_df)\n",
    "\n",
    "# Verify assembled features\n",
    "train_df.select(\"features\").show(5, truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataset if needed\n",
    "train_df, test_df = train_df.randomSplit([0.8, 0.2], seed=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+------------------------------------------+\n",
      "|is_fraud|prediction|probability                               |\n",
      "+--------+----------+------------------------------------------+\n",
      "|0       |0.0       |[0.9849165636632622,0.01508343633673781]  |\n",
      "|0       |0.0       |[0.9973706242638098,0.0026293757361901537]|\n",
      "|0       |0.0       |[0.9962710139264893,0.0037289860735106872]|\n",
      "|0       |0.0       |[0.994004059840046,0.005995940159953972]  |\n",
      "|0       |0.0       |[0.9932827313393577,0.006717268660642262] |\n",
      "+--------+----------+------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "# Define the Logistic Regression model\n",
    "lr = LogisticRegression(featuresCol=\"features\", labelCol=\"is_fraud\")\n",
    "\n",
    "# Train the model\n",
    "lr_model = lr.fit(train_df)\n",
    "\n",
    "# Evaluate on test set\n",
    "predictions = lr_model.transform(test_df)\n",
    "\n",
    "# View predictions\n",
    "predictions.select(\"is_fraud\", \"prediction\", \"probability\").show(5, truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building the Transformer Model Architecture\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class FraudDetectionTransformer(nn.Module):\n",
    "    def __init__(self, input_dim, n_heads, n_layers, hidden_dim, output_dim, dropout=0.1):\n",
    "        super(FraudDetectionTransformer, self).__init__()\n",
    "        \n",
    "        self.embedding = nn.Linear(input_dim, hidden_dim)\n",
    "        self.positional_encoding = nn.Parameter(torch.zeros(1, 500, hidden_dim))  # Adjust 500 if sequence length varies\n",
    "        \n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=hidden_dim, nhead=n_heads, dim_feedforward=hidden_dim * 2, dropout=dropout\n",
    "        )\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=n_layers)\n",
    "        \n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim // 2, output_dim),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        seq_len, batch_size, _ = x.size()\n",
    "        x = self.embedding(x)\n",
    "        x += self.positional_encoding[:, :seq_len, :]\n",
    "        x = self.transformer(x)\n",
    "        x = x.mean(dim=1)  # Pool across sequence length\n",
    "        return self.fc(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Drop non-numerical columns and target column from features\n",
    "features = train_df.select(\n",
    "    ['amt', 'city_pop', 'lat', 'long', 'unix_time', 'merch_lat', 'merch_long', 'merchant_index', 'category_index']\n",
    ").toPandas()\n",
    "\n",
    "labels = train_df.select('is_fraud').toPandas()\n",
    "\n",
    "# Convert to NumPy arrays\n",
    "train_sequences = features.values\n",
    "train_labels = labels.values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_features = test_df.select(\n",
    "    ['amt', 'city_pop', 'lat', 'long', 'unix_time', 'merch_lat', 'merch_long', 'merchant_index', 'category_index']\n",
    ").toPandas()\n",
    "\n",
    "val_labels = test_df.select('is_fraud').toPandas()\n",
    "\n",
    "val_sequences = val_features.values\n",
    "val_labels = val_labels.values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class FraudDataset(Dataset):\n",
    "    def __init__(self, sequences, labels):\n",
    "        self.sequences = sequences\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.sequences[idx], dtype=torch.float32), torch.tensor(self.labels[idx], dtype=torch.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Create Dataset objects\n",
    "train_dataset = FraudDataset(train_sequences, train_labels)\n",
    "val_dataset = FraudDataset(val_sequences, val_labels)\n",
    "\n",
    "# Create DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class FraudDataset(Dataset):\n",
    "    def __init__(self, sequences, labels):\n",
    "        self.sequences = sequences\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Add a dummy sequence dimension (sequence length = 1)\n",
    "        sequence = torch.tensor(self.sequences[idx], dtype=torch.float32).unsqueeze(0)\n",
    "        label = torch.tensor(self.labels[idx], dtype=torch.float32)\n",
    "        return sequence, label\n",
    "\n",
    "\n",
    "# Example: Assume train_seq and test_seq are lists of sequences and labels\n",
    "train_dataset = FraudDataset(train_sequences, train_labels)\n",
    "val_dataset = FraudDataset(val_sequences, val_labels)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, n_heads, n_layers):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        self.embedding = nn.Linear(input_dim, hidden_dim)  # Embed input features\n",
    "        self.encoder_layer = nn.TransformerEncoderLayer(d_model=hidden_dim, nhead=n_heads)\n",
    "        self.transformer = nn.TransformerEncoder(self.encoder_layer, num_layers=n_layers)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)  # Final classification layer\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: (batch_size, seq_len, feature_dim)\n",
    "        seq_len = x.size(1)\n",
    "\n",
    "        # Embed the input features\n",
    "        x = self.embedding(x)\n",
    "\n",
    "        # Skip positional encoding if seq_len = 1\n",
    "        if seq_len > 1:\n",
    "            positional_encoding = torch.zeros(x.size(), device=x.device)\n",
    "            for pos in range(seq_len):\n",
    "                positional_encoding[:, pos, :] = torch.arange(self.hidden_dim, device=x.device)\n",
    "            x += positional_encoding\n",
    "\n",
    "        # Transformer expects input of shape (seq_len, batch_size, hidden_dim)\n",
    "        x = x.transpose(0, 1)\n",
    "        x = self.transformer(x)\n",
    "        x = x.mean(dim=0)  # Pool across sequence length\n",
    "        return torch.sigmoid(self.fc(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Train Loss: 0.0359, Val Loss: 0.0361\n",
      "Epoch 2, Train Loss: 0.0358, Val Loss: 0.0357\n",
      "Epoch 3, Train Loss: 0.0358, Val Loss: 0.0361\n",
      "Epoch 4, Train Loss: 0.0358, Val Loss: 0.0357\n",
      "Epoch 5, Train Loss: 0.0358, Val Loss: 0.0359\n",
      "Epoch 6, Train Loss: 0.0358, Val Loss: 0.0361\n",
      "Epoch 7, Train Loss: 0.0357, Val Loss: 0.0358\n",
      "Epoch 8, Train Loss: 0.0357, Val Loss: 0.0365\n",
      "Epoch 9, Train Loss: 0.0357, Val Loss: 0.0361\n",
      "Epoch 10, Train Loss: 0.0357, Val Loss: 0.0361\n"
     ]
    }
   ],
   "source": [
    "# Adjust input_dim to match your dataset\n",
    "model = TransformerModel(input_dim=9, hidden_dim=128, output_dim=1, n_heads=4, n_layers=2)\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Train the model\n",
    "train_transformer(model, train_loader, val_loader, optimizer, criterion, epochs=10, device=device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "deep_features = []\n",
    "with torch.no_grad():\n",
    "    for X_batch, _ in train_loader:\n",
    "        X_batch = X_batch.to(device)\n",
    "        outputs = model(X_batch)\n",
    "        deep_features.append(outputs.cpu().numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
